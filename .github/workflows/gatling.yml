name: Load Test

on:
  push:
    branches: [ master ]
  pull_request:
    branches: [ '**' ]

permissions:
  contents: read
  pull-requests: write

jobs:
  gatling:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
    steps:
      - uses: actions/checkout@v6
      - name: Run Smoke Tests
        if: always()
        run: (cd gatling; GATLING_TEST=SmokeTest docker compose --profile lt up --build --exit-code-from gatling)
      - name: Run Comprehensive Tests
        if: always()
        run: (cd gatling; GATLING_TEST=Comprehensive docker compose --profile lt up --build --exit-code-from gatling)
      - name: Run User Journey
        if: always()
        run: (cd gatling; GATLING_TEST=UserJourney docker compose --profile lt up --build --exit-code-from gatling)
      - name: Run Stress Tests
        if: always()
        run: (cd gatling; GATLING_TEST=StressTest docker compose --profile lt up --build --exit-code-from gatling)
      - name: Run Inferno Tests
        if: always()
        run: (cd gatling; GATLING_TEST=Inferno docker compose --profile lt up --build --exit-code-from gatling)
      - name: Generate Report Index
        if: always()
        run: (cd gatling; sudo chown $(whoami) -R report; ./gen-index.sh)
      - name: Generate Load Test Summary
        if: always()
        id: load_test_summary
        run: |
          # This script runs in bash (default shell for GitHub Actions run steps)
          cd gatling
          
          # Create Python script for parsing HTML table stats
          cat > /tmp/parse_stats.py << 'PYTHON_EOF'
          import sys
          import re
          
          try:
              with open(sys.argv[1], 'r') as f:
                  content = f.read()
              
              # Parse HTML file - extract stats from table rows
              # Gatling 3.14.9+ embeds stats directly in HTML tables, not as JavaScript variables
              # Look for the "All Requests" row in the statistics table
              # Format: <tr id="ROOT" >...<td class="value total col-2">169</td>...
              
              # Find the ROOT table row (contains "All Requests")
              root_row_match = re.search(r'<tr\s+id="ROOT"[^>]*>(.*?)</tr>', content, re.DOTALL | re.IGNORECASE)
              if not root_row_match:
                  print(f"ERROR: Could not find ROOT statistics row in HTML", file=sys.stderr)
                  print(f"0|0|0|0.0|0|0|0")
                  sys.exit(0)
              
              row_content = root_row_match.group(1)
              
              # Extract values from table cells
              # col-2: Total requests, col-3: OK, col-4: KO, col-5: % KO
              # col-13: Mean time, col-10: P95, col-11: P99
              
              def extract_cell_value(col_num):
                  """Extract numeric value from a table cell"""
                  # Capture full inner HTML of the target <td> (including any nested tags)
                  pattern = rf'(?s)<td[^>]*class="[^"]*col-{col_num}"[^>]*>(.*?)</td>'
                  match = re.search(pattern, row_content)
                  if match:
                      cell_html = match.group(1)
                      # Strip any nested HTML tags before extracting the numeric value
                      value_str = re.sub(r'<[^>]*>', '', cell_html).strip()
                      # Remove any non-numeric characters except dots and minus
                      value_str = re.sub(r'[^\d.-]', '', value_str)
                      try:
                          return float(value_str) if '.' in value_str else int(value_str)
                      except ValueError:
                          return 0
                  return 0
              
              total = extract_cell_value(2)     # Total requests
              ok = extract_cell_value(3)        # OK requests
              ko = extract_cell_value(4)        # KO (failed) requests
              mean_time = extract_cell_value(13)  # Mean response time
              p95 = extract_cell_value(10)      # 95th percentile
              p99 = extract_cell_value(11)      # 99th percentile
              
              # Calculate success rate
              success_rate = (ok / total * 100) if total > 0 else 0
              
              print(f"DEBUG: Extracted from HTML table - Total: {total}, OK: {ok}, KO: {ko}, Mean: {mean_time}, P95: {p95}, P99: {p99}", file=sys.stderr)
              
              # Output in a parseable format
              print(f"{total}|{ok}|{ko}|{success_rate:.1f}|{mean_time}|{p95}|{p99}")
          except Exception as e:
              print(f"ERROR: {e}", file=sys.stderr)
              import traceback
              traceback.print_exc(file=sys.stderr)
              print(f"0|0|0|0.0|0|0|0")
          PYTHON_EOF
          
          # Initialize summary variables
          TOTAL_REQUESTS=0
          SUCCESSFUL_REQUESTS=0
          FAILED_REQUESTS=0
          declare -A SIMULATION_STATS
          
          # Parse each simulation's HTML report (index.html) to extract table statistics
          echo "Looking for index.html files in report directory..."
          echo "Current directory: $(pwd)"
          echo "Report directory exists: $([ -d report ] && echo 'yes' || echo 'no')"
          if [ -d report ]; then
            echo "Report directory contents:"
            ls -la report/
            
            echo ""
            echo "Exploring subdirectories..."
            for sim_dir in report/*/; do
              if [ -d "$sim_dir" ]; then
                echo "Contents of $sim_dir:"
                ls -la "$sim_dir" 2>/dev/null || echo "  Cannot list $sim_dir"
                
                # Check for js subdirectory
                if [ -d "${sim_dir}js" ]; then
                  echo "  Contents of ${sim_dir}js:"
                  ls -la "${sim_dir}js" 2>/dev/null || echo "    Cannot list ${sim_dir}js"
                else
                  echo "  No js subdirectory in $sim_dir"
                fi
              fi
            done
          fi
          
          echo ""
          echo "Running find command for index.html files..."
          stats_files=$(find report -maxdepth 3 -type f -name "index.html" 2>&1)
          echo "Find command output:"
          echo "$stats_files"
          
          echo ""
          echo "Found index.html files:"
          stats_files=$(find report -maxdepth 3 -type f -name "index.html" 2>/dev/null)
          echo "$stats_files"
          
          if [ -z "$stats_files" ]; then
            echo "WARNING: No index.html files found!"
          fi
          
          printf '%s\n' "$stats_files" | while IFS= read -r stats_file; do
            if [ -f "$stats_file" ]; then
              echo "Processing $stats_file"
              
              # Extract simulation name from path
              # Path structure: report/<simulation-name-timestamp>/index.html
              # Go up one level from index.html to get the simulation directory
              sim_dir=$(dirname "$stats_file")
              sim_name=$(basename "$sim_dir")
              
              # Sanitize simulation name for use in file paths (allow alnum, underscore, hyphen, and period)
              safe_sim_name=$(echo "$sim_name" | tr -cd '[:alnum:]_.-')
              
              echo "Simulation name: $sim_name (sanitized: $safe_sim_name)"
              
              # Parse HTML Gatling report (index.html) using Python script (show errors for debugging)
              # Keep stdout (data) separate from stderr (debug) to avoid mixing debug lines with parsed data
              python3 /tmp/parse_stats.py "$stats_file" > "/tmp/parsed_stats_${safe_sim_name}.txt" 2> "/tmp/parsed_stats_${safe_sim_name}_debug.txt" || echo "0|0|0|0.0|0|0|0" > "/tmp/parsed_stats_${safe_sim_name}.txt"
              
              # Show what was parsed (for debugging)
              echo "Parsed data output:"
              cat "/tmp/parsed_stats_${safe_sim_name}.txt" || echo "No parsed data file found"
              
              # Show any debug or error output from the parser
              if [ -s "/tmp/parsed_stats_${safe_sim_name}_debug.txt" ]; then
                echo "Parser debug output (stderr):"
                cat "/tmp/parsed_stats_${safe_sim_name}_debug.txt"
              else
                echo "No parser debug output"
              fi
              
              # Read parsed stats from the data line
              if [ -f "/tmp/parsed_stats_${safe_sim_name}.txt" ]; then
                IFS='|' read -r total ok ko success_rate mean_time p95 p99 < "/tmp/parsed_stats_${safe_sim_name}.txt"
                
                echo "Extracted values: total='$total', ok='$ok', ko='$ko', success_rate='$success_rate', mean_time='$mean_time', p95='$p95', p99='$p99'"
                
                # Validate that we got numeric values for counts
                if [ -z "$total" ] || [ -z "$ok" ] || [ -z "$ko" ]; then
                  echo "ERROR: Missing count values in parsed output for simulation '$sim_name'"
                  continue
                fi
                
                # Ensure count fields are numeric to avoid bash arithmetic errors
                if ! [[ "$total" =~ ^[0-9]+$ ]] || ! [[ "$ok" =~ ^[0-9]+$ ]] || ! [[ "$ko" =~ ^[0-9]+$ ]]; then
                  echo "ERROR: Non-numeric count values in parsed output for simulation '$sim_name': total='$total', ok='$ok', ko='$ko'"
                  continue
                fi
                
                # Store simulation stats (use original name for display)
                SIMULATION_STATS[$sim_name]="${total}|${ok}|${ko}|${success_rate}|${mean_time}|${p95}|${p99}"
                
                # Aggregate totals (with defaults to avoid arithmetic errors)
                TOTAL_REQUESTS=$((TOTAL_REQUESTS + ${total:-0}))
                SUCCESSFUL_REQUESTS=$((SUCCESSFUL_REQUESTS + ${ok:-0}))
                FAILED_REQUESTS=$((FAILED_REQUESTS + ${ko:-0}))
                
                echo "Simulation: $sim_name - Total: $total, OK: $ok, KO: $ko, Success: ${success_rate}%"
                echo "Running totals - Total: $TOTAL_REQUESTS, OK: $SUCCESSFUL_REQUESTS, KO: $FAILED_REQUESTS"
              fi
            fi
          done
          
          echo "Final aggregated values:"
          echo "TOTAL_REQUESTS=$TOTAL_REQUESTS"
          echo "SUCCESSFUL_REQUESTS=$SUCCESSFUL_REQUESTS"
          echo "FAILED_REQUESTS=$FAILED_REQUESTS"
          
          # Calculate overall success rate
          if [ $TOTAL_REQUESTS -gt 0 ]; then
            SUCCESS_RATE=$(awk "BEGIN {printf \"%.1f\", ($SUCCESSFUL_REQUESTS / $TOTAL_REQUESTS) * 100}")
          else
            SUCCESS_RATE="0.0"
          fi
          
          # Set outputs
          echo "total_requests=$TOTAL_REQUESTS" >> $GITHUB_OUTPUT
          echo "successful_requests=$SUCCESSFUL_REQUESTS" >> $GITHUB_OUTPUT
          echo "failed_requests=$FAILED_REQUESTS" >> $GITHUB_OUTPUT
          echo "success_rate=$SUCCESS_RATE" >> $GITHUB_OUTPUT
          
          # Determine if tests passed (>70% success for stress tests, >75% for others)
          # Use awk for arithmetic comparison (more portable than bc)
          if [ $(awk -v sr="$SUCCESS_RATE" 'BEGIN { print (sr >= 70.0) ? 1 : 0 }') -eq 1 ]; then
            echo "tests_passed=true" >> $GITHUB_OUTPUT
          else
            echo "tests_passed=false" >> $GITHUB_OUTPUT
          fi
          
          # Export simulation stats as JSON for the comment script
          echo "simulation_stats<<EOF" >> $GITHUB_OUTPUT
          for sim in $(printf '%s\n' "${!SIMULATION_STATS[@]}" | sort); do
            echo "$sim:${SIMULATION_STATS[$sim]}"
          done
          echo "EOF" >> $GITHUB_OUTPUT
          
          # Create workflow summary
          echo "## Load Test Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Count |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Total Requests | $TOTAL_REQUESTS |" >> $GITHUB_STEP_SUMMARY
          echo "| âœ… Successful | $SUCCESSFUL_REQUESTS |" >> $GITHUB_STEP_SUMMARY
          echo "| âŒ Failed | $FAILED_REQUESTS |" >> $GITHUB_STEP_SUMMARY
          echo "| ðŸ“Š Success Rate | ${SUCCESS_RATE}% |" >> $GITHUB_STEP_SUMMARY
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Per-Simulation Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ ${#SIMULATION_STATS[@]} -eq 0 ]; then
            echo "_No per-simulation results could be generated. This usually means all simulation reports failed to parse. Please check the Gatling reports and logs for details._" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Simulation | Requests | Success Rate | Mean (ms) | P95 (ms) | P99 (ms) |" >> $GITHUB_STEP_SUMMARY
            echo "|------------|----------|--------------|-----------|----------|----------|" >> $GITHUB_STEP_SUMMARY
            
            for sim in $(printf '%s\n' "${!SIMULATION_STATS[@]}" | sort); do
              IFS='|' read -r total ok ko success_rate mean_time p95 p99 <<< "${SIMULATION_STATS[$sim]}"
              echo "| $sim | $total | ${success_rate}% | $mean_time | $p95 | $p99 |" >> $GITHUB_STEP_SUMMARY
            done
          fi
      - name: Upload PR Report
        if: github.event_name == 'pull_request' && always()
        uses: actions/upload-artifact@v6
        with:
          name: gatling-reports-pr-${{ github.event.pull_request.number }}
          path: gatling/report
          retention-days: 7
      - name: Upload Reports
        if: github.event_name != 'pull_request' && always()
        uses: actions/upload-artifact@v6
        with:
          name: gatling-reports-${{ github.ref_name }}-${{ github.run_number }}
          path: gatling/report
          retention-days: 90
      - name: Comment on PR with Load Test Summary
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v8
        with:
          # language=JavaScript
          script: |
            const fs = require('fs');
            const path = require('path');
            
            const totalRequests = '${{ steps.load_test_summary.outputs.total_requests }}' || '0';
            const successfulRequests = '${{ steps.load_test_summary.outputs.successful_requests }}' || '0';
            const failedRequests = '${{ steps.load_test_summary.outputs.failed_requests }}' || '0';
            const successRate = '${{ steps.load_test_summary.outputs.success_rate }}' || '0.0';
            const testsPassed = '${{ steps.load_test_summary.outputs.tests_passed }}' === 'true';
            
            let emoji = 'âœ…';
            let status = 'All load tests passed!';
            
            if (!testsPassed) {
              emoji = 'âš ï¸';
              status = 'Load tests completed with warnings!';
            }
            
            let comment = `## ${emoji} Load Test Results\n\n`;
            comment += `**${status}**\n\n`;
            comment += `### Overall Summary\n\n`;
            comment += `| Metric | Value |\n`;
            comment += `|--------|-------|\n`;
            comment += `| Total Requests | ${totalRequests} |\n`;
            comment += `| âœ… Successful | ${successfulRequests} |\n`;
            comment += `| âŒ Failed | ${failedRequests} |\n`;
            comment += `| ðŸ“Š Success Rate | ${successRate}% |\n\n`;
            
            // Parse simulation stats
            const simulationStats = '${{ steps.load_test_summary.outputs.simulation_stats }}';
            if (simulationStats && simulationStats !== '') {
              comment += `### Per-Simulation Results\n\n`;
              comment += `| Simulation | Requests | Success Rate | Mean Time | P95 | P99 |\n`;
              comment += `|------------|----------|--------------|-----------|-----|-----|\n`;
              
              const simLines = simulationStats.split('\n').filter(line => line.trim() !== '');
              for (const line of simLines) {
                const [simName, statsStr] = line.split(':');
                if (statsStr) {
                  const [total, ok, ko, success_rate, mean_time, p95, p99] = statsStr.split('|');
                  
                  // Format simulation name (remove timestamp suffix for readability)
                  let displayName = simName.replace(/simulation-\d+/i, '').replace(/-/g, ' ').trim();
                  if (displayName.toLowerCase().includes('smoketest')) {
                    displayName = 'ðŸ”¥ Smoke Test';
                  } else if (displayName.toLowerCase().includes('comprehensive')) {
                    displayName = 'ðŸ“‹ Comprehensive';
                  } else if (displayName.toLowerCase().includes('userjourney')) {
                    displayName = 'ðŸ‘¤ User Journey';
                  } else if (displayName.toLowerCase().includes('stresstest')) {
                    displayName = 'ðŸ’ª Stress Test';
                  } else if (displayName.toLowerCase().includes('inferno')) {
                    displayName = 'ðŸ”¥ Inferno';
                  }
                  
                  comment += `| ${displayName} | ${total} | ${success_rate}% | ${mean_time}ms | ${p95}ms | ${p99}ms |\n`;
                }
              }
              comment += `\n`;
            }
            
            // Add link to detailed report
            const url = `https://${context.repo.owner}.github.io/${context.repo.repo}/reports/gatling-reports-pr-${context.issue.number}/`;
            comment += `ðŸ“ˆ [View detailed performance reports](${url})\n\n`;
            
            // Add performance criteria info
            comment += `<details>\n<summary>Performance Criteria</summary>\n\n`;
            comment += `- **Smoke Test**: >75% success rate, <5s max response time, <1.5s mean\n`;
            comment += `- **Comprehensive**: >75% success rate, <8s max response time, <2s mean\n`;
            comment += `- **User Journey**: >75% success rate, <10s max response time, <3s mean\n`;
            comment += `- **Stress Test**: >70% success rate, <15s max response time, <5s mean\n`;
            comment += `- **Inferno**: >15% success rate (extreme load scenario)\n`;
            comment += `</details>\n\n`;
            
            // Add marker to identify our comment
            comment += `<!-- gatling-results-comment -->`;
            
            // Find existing comment and update it, or create new one
            const comments = await github.paginate(
              github.rest.issues.listComments,
              {
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
              }
            );
            
            const botComment = comments.find(comment => 
              comment.body.includes('<!-- gatling-results-comment -->')
            );
            
            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            }

  clean:
    needs: gatling
    if: always()
    runs-on: ubuntu-latest
    permissions:
      actions: write
    steps:
      - name: Cleanup old artifacts
        if: github.event_name != 'pull_request'
        uses: actions/github-script@v8
        with:
          # language=JavaScript
          script: |
            const maxVersions = 5;
            const response = await github.rest.actions.listArtifactsForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              per_page: 100
            });
            const artifacts = response.data.artifacts
              .filter(artifact => artifact.name.startsWith('gatling-reports-' + '${{ github.ref_name }}'));
            if (artifacts.length > maxVersions) {
              // Sort by created_at descending
              artifacts.sort((a, b) => new Date(b.created_at) - new Date(a.created_at));
              // Delete all but the latest maxVersions
              for (const artifact of artifacts.slice(maxVersions)) {
                console.log('Deleting artifact:', artifact.name);
                await github.rest.actions.deleteArtifact({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  artifact_id: artifact.id
                });
              }
            }
