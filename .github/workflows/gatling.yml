name: Load Test

on:
  push:
    branches: [ master ]
  pull_request:
    branches: [ '**' ]

permissions:
  contents: read
  pull-requests: write

jobs:
  gatling:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
    steps:
      - uses: actions/checkout@v6
      - name: Run Smoke Tests
        if: always()
        run: (cd gatling; GATLING_TEST=SmokeTest docker compose --profile lt up --build --exit-code-from gatling)
      - name: Run Comprehensive Tests
        if: always()
        run: (cd gatling; GATLING_TEST=Comprehensive docker compose --profile lt up --build --exit-code-from gatling)
      - name: Run User Journey
        if: always()
        run: (cd gatling; GATLING_TEST=UserJourney docker compose --profile lt up --build --exit-code-from gatling)
      - name: Run Stress Tests
        if: always()
        run: (cd gatling; GATLING_TEST=StressTest docker compose --profile lt up --build --exit-code-from gatling)
      - name: Run Inferno Tests
        if: always()
        run: (cd gatling; GATLING_TEST=Inferno docker compose --profile lt up --build --exit-code-from gatling)
      - name: Generate Report Index
        if: always()
        run: (cd gatling; sudo chown $(whoami) -R report; ./gen-index.sh)
      - name: Generate Load Test Summary
        if: always()
        id: load_test_summary
        run: |
          # This script runs in bash (default shell for GitHub Actions run steps)
          cd gatling
          
          # Create Python script for parsing stats.json
          cat > /tmp/parse_stats.py << 'PYTHON_EOF'
          import json
          import sys
          import re
          
          try:
              with open(sys.argv[1], 'r') as f:
                  content = f.read()
              
              # Check if this is an HTML file (index.html) or JS file (stats.js)
              if sys.argv[1].endswith('.html'):
                  # Parse HTML file - extract JSON from script tag
                  # Gatling embeds stats in: var stats = { ... };
                  # or window.stats = { ... }; inside a <script> tag
                  # Use greedy match to capture the entire object
                  match = re.search(r'(?:var|const|let|window\.)\s*stats\s*=\s*(\{.*\});', content, re.DOTALL)
                  if not match:
                      print(f"ERROR: Could not find stats variable in HTML file", file=sys.stderr)
                      print(f"DEBUG: Searching in HTML content (length: {len(content)})", file=sys.stderr)
                      print(f"0|0|0|0.0|0|0|0")
                      sys.exit(0)
                  
                  json_str = match.group(1)
                  data = json.loads(json_str)
              elif sys.argv[1].endswith('.js'):
                  # Parse JavaScript file - extract JSON from variable assignment
                  # Gatling stats.js format: var stats = { ... }; or const stats = { ... };
                  # Use greedy match to capture the entire object
                  match = re.search(r'(?:var|const|let)\s+stats\s*=\s*(\{.*\});', content, re.DOTALL)
                  if not match:
                      print(f"ERROR: Could not find stats variable in JS file", file=sys.stderr)
                      print(f"DEBUG: File starts with: {content[:200]}", file=sys.stderr)
                      print(f"0|0|0|0.0|0|0|0")
                      sys.exit(0)
                  
                  json_str = match.group(1)
                  data = json.loads(json_str)
              else:
                  # Parse JSON file directly
                  data = json.loads(content)
              
              # Debug: print structure to stderr for debugging
              print(f"DEBUG: Top-level keys: {list(data.keys())}", file=sys.stderr)
              
              # Get global stats
              stats = data.get('stats', {})
              print(f"DEBUG: Stats keys: {list(stats.keys())}", file=sys.stderr)
              
              global_stats = stats.get('global', {})
              if not global_stats:
                  print(f"DEBUG: No 'global' key in stats", file=sys.stderr)
                  print(f"0|0|0|0.0|0|0|0")
                  sys.exit(0)
              
              print(f"DEBUG: Global stats keys: {list(global_stats.keys())}", file=sys.stderr)
              
              # Extract request counts
              requests = global_stats.get('numberOfRequests', {})
              total = requests.get('total', 0)
              ok = requests.get('ok', 0)
              ko = requests.get('ko', 0)
              
              print(f"DEBUG: Requests - total: {total}, ok: {ok}, ko: {ko}", file=sys.stderr)
              
              # Extract response time percentiles
              response_time = global_stats.get('meanResponseTime', {})
              mean_time = response_time.get('total', 0)
              
              # Gatling percentiles: 1=50th, 2=75th, 3=95th, 4=99th
              percentiles3 = global_stats.get('percentiles3', {})
              p95 = percentiles3.get('total', 0)
              
              percentiles4 = global_stats.get('percentiles4', {})
              p99 = percentiles4.get('total', 0)
              
              # Calculate success rate
              success_rate = (ok / total * 100) if total > 0 else 0
              
              # Output in a parseable format
              print(f"{total}|{ok}|{ko}|{success_rate:.1f}|{mean_time}|{p95}|{p99}")
          except Exception as e:
              print(f"ERROR: {e}", file=sys.stderr)
              import traceback
              traceback.print_exc(file=sys.stderr)
              print(f"0|0|0|0.0|0|0|0")
          PYTHON_EOF
          
          # Initialize summary variables
          TOTAL_REQUESTS=0
          SUCCESSFUL_REQUESTS=0
          FAILED_REQUESTS=0
          declare -A SIMULATION_STATS
          
          # Parse each simulation's stats.json
          echo "Looking for stats.json files in report directory..."
          echo "Current directory: $(pwd)"
          echo "Report directory exists: $([ -d report ] && echo 'yes' || echo 'no')"
          if [ -d report ]; then
            echo "Report directory contents:"
            ls -la report/
            
            echo ""
            echo "Exploring subdirectories..."
            for sim_dir in report/*/; do
              if [ -d "$sim_dir" ]; then
                echo "Contents of $sim_dir:"
                ls -la "$sim_dir" 2>/dev/null || echo "  Cannot list $sim_dir"
                
                # Check for js subdirectory
                if [ -d "${sim_dir}js" ]; then
                  echo "  Contents of ${sim_dir}js:"
                  ls -la "${sim_dir}js" 2>/dev/null || echo "    Cannot list ${sim_dir}js"
                else
                  echo "  No js subdirectory in $sim_dir"
                fi
              fi
            done
          fi
          
          echo ""
          echo "Running find command for index.html files..."
          stats_files=$(find report -maxdepth 2 -type f -name "index.html" 2>&1)
          echo "Find command output:"
          echo "$stats_files"
          
          echo ""
          echo "Found index.html files:"
          stats_files=$(find report -maxdepth 2 -type f -name "index.html" 2>/dev/null)
          echo "$stats_files"
          
          if [ -z "$stats_files" ]; then
            echo "WARNING: No index.html files found!"
          fi
          
          for stats_file in $stats_files; do
            if [ -f "$stats_file" ]; then
              echo "Processing $stats_file"
              
              # Extract simulation name from path
              # Path structure: report/<simulation-name-timestamp>/index.html
              # Go up one level from index.html to get the simulation directory
              sim_dir=$(dirname "$stats_file")
              sim_name=$(basename "$sim_dir")
              
              # Sanitize simulation name for use in file paths (remove special chars)
              safe_sim_name=$(echo "$sim_name" | tr -cd '[:alnum:]_-')
              
              echo "Simulation name: $sim_name (sanitized: $safe_sim_name)"
              
              # Parse stats.json using Python script (show errors for debugging)
              # Redirect both stdout and stderr to capture debug info, then extract last line for data
              python3 /tmp/parse_stats.py "$stats_file" > "/tmp/parsed_stats_${safe_sim_name}_full.txt" 2>&1 || echo "0|0|0|0.0|0|0|0" > "/tmp/parsed_stats_${safe_sim_name}_full.txt"
              
              # Show what was parsed (for debugging)
              echo "Parsed output (full):"
              cat "/tmp/parsed_stats_${safe_sim_name}_full.txt"
              
              # Extract only the last line (the actual data, not debug messages)
              tail -1 "/tmp/parsed_stats_${safe_sim_name}_full.txt" > "/tmp/parsed_stats_${safe_sim_name}.txt"
              
              echo "Data line only:"
              cat "/tmp/parsed_stats_${safe_sim_name}.txt"
              
              # Read parsed stats from the data line
              if [ -f "/tmp/parsed_stats_${safe_sim_name}.txt" ]; then
                IFS='|' read -r total ok ko success_rate mean_time p95 p99 < "/tmp/parsed_stats_${safe_sim_name}.txt"
                
                echo "Extracted values: total='$total', ok='$ok', ko='$ko', success_rate='$success_rate', mean_time='$mean_time', p95='$p95', p99='$p99'"
                
                # Validate that we got numeric values
                if [ -z "$total" ] || [ -z "$ok" ] || [ -z "$ko" ]; then
                  echo "ERROR: Failed to extract numeric values from parsed output"
                  continue
                fi
                
                # Store simulation stats (use original name for display)
                SIMULATION_STATS[$sim_name]="${total}|${ok}|${ko}|${success_rate}|${mean_time}|${p95}|${p99}"
                
                # Aggregate totals (with defaults to avoid arithmetic errors)
                TOTAL_REQUESTS=$((TOTAL_REQUESTS + ${total:-0}))
                SUCCESSFUL_REQUESTS=$((SUCCESSFUL_REQUESTS + ${ok:-0}))
                FAILED_REQUESTS=$((FAILED_REQUESTS + ${ko:-0}))
                
                echo "Simulation: $sim_name - Total: $total, OK: $ok, KO: $ko, Success: ${success_rate}%"
                echo "Running totals - Total: $TOTAL_REQUESTS, OK: $SUCCESSFUL_REQUESTS, KO: $FAILED_REQUESTS"
              fi
            fi
          done
          
          echo "Final aggregated values:"
          echo "TOTAL_REQUESTS=$TOTAL_REQUESTS"
          echo "SUCCESSFUL_REQUESTS=$SUCCESSFUL_REQUESTS"
          echo "FAILED_REQUESTS=$FAILED_REQUESTS"
          
          # Calculate overall success rate
          if [ $TOTAL_REQUESTS -gt 0 ]; then
            SUCCESS_RATE=$(awk "BEGIN {printf \"%.1f\", ($SUCCESSFUL_REQUESTS / $TOTAL_REQUESTS) * 100}")
          else
            SUCCESS_RATE="0.0"
          fi
          
          # Set outputs
          echo "total_requests=$TOTAL_REQUESTS" >> $GITHUB_OUTPUT
          echo "successful_requests=$SUCCESSFUL_REQUESTS" >> $GITHUB_OUTPUT
          echo "failed_requests=$FAILED_REQUESTS" >> $GITHUB_OUTPUT
          echo "success_rate=$SUCCESS_RATE" >> $GITHUB_OUTPUT
          
          # Determine if tests passed (>70% success for stress tests, >75% for others)
          # Use awk for arithmetic comparison (more portable than bc)
          if [ $(awk -v sr="$SUCCESS_RATE" 'BEGIN { print (sr >= 70.0) ? 1 : 0 }') -eq 1 ]; then
            echo "tests_passed=true" >> $GITHUB_OUTPUT
          else
            echo "tests_passed=false" >> $GITHUB_OUTPUT
          fi
          
          # Export simulation stats as JSON for the comment script
          echo "simulation_stats<<EOF" >> $GITHUB_OUTPUT
          for sim in "${!SIMULATION_STATS[@]}"; do
            echo "$sim:${SIMULATION_STATS[$sim]}"
          done
          echo "EOF" >> $GITHUB_OUTPUT
          
          # Create workflow summary
          echo "## Load Test Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Count |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Total Requests | $TOTAL_REQUESTS |" >> $GITHUB_STEP_SUMMARY
          echo "| âœ… Successful | $SUCCESSFUL_REQUESTS |" >> $GITHUB_STEP_SUMMARY
          echo "| âŒ Failed | $FAILED_REQUESTS |" >> $GITHUB_STEP_SUMMARY
          echo "| ðŸ“Š Success Rate | ${SUCCESS_RATE}% |" >> $GITHUB_STEP_SUMMARY
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Per-Simulation Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Simulation | Requests | Success Rate | Mean (ms) | P95 (ms) | P99 (ms) |" >> $GITHUB_STEP_SUMMARY
          echo "|------------|----------|--------------|-----------|----------|----------|" >> $GITHUB_STEP_SUMMARY
          
          for sim in "${!SIMULATION_STATS[@]}"; do
            IFS='|' read -r total ok ko success_rate mean_time p95 p99 <<< "${SIMULATION_STATS[$sim]}"
            echo "| $sim | $total | ${success_rate}% | $mean_time | $p95 | $p99 |" >> $GITHUB_STEP_SUMMARY
          done
      - name: Upload PR Report
        if: github.event_name == 'pull_request' && always()
        uses: actions/upload-artifact@v6
        with:
          name: gatling-reports-pr-${{ github.event.pull_request.number }}
          path: gatling/report
          retention-days: 7
      - name: Upload Reports
        if: github.event_name != 'pull_request' && always()
        uses: actions/upload-artifact@v6
        with:
          name: gatling-reports-${{ github.ref_name }}-${{ github.run_number }}
          path: gatling/report
          retention-days: 90
      - name: Comment on PR with Load Test Summary
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v8
        with:
          # language=JavaScript
          script: |
            const fs = require('fs');
            const path = require('path');
            
            const totalRequests = '${{ steps.load_test_summary.outputs.total_requests }}' || '0';
            const successfulRequests = '${{ steps.load_test_summary.outputs.successful_requests }}' || '0';
            const failedRequests = '${{ steps.load_test_summary.outputs.failed_requests }}' || '0';
            const successRate = '${{ steps.load_test_summary.outputs.success_rate }}' || '0.0';
            const testsPassed = '${{ steps.load_test_summary.outputs.tests_passed }}' === 'true';
            
            let emoji = 'âœ…';
            let status = 'All load tests passed!';
            
            if (!testsPassed) {
              emoji = 'âš ï¸';
              status = 'Load tests completed with warnings!';
            }
            
            let comment = `## ${emoji} Load Test Results\n\n`;
            comment += `**${status}**\n\n`;
            comment += `### Overall Summary\n\n`;
            comment += `| Metric | Value |\n`;
            comment += `|--------|-------|\n`;
            comment += `| Total Requests | ${totalRequests} |\n`;
            comment += `| âœ… Successful | ${successfulRequests} |\n`;
            comment += `| âŒ Failed | ${failedRequests} |\n`;
            comment += `| ðŸ“Š Success Rate | ${successRate}% |\n\n`;
            
            // Parse simulation stats
            const simulationStats = '${{ steps.load_test_summary.outputs.simulation_stats }}';
            if (simulationStats && simulationStats !== '') {
              comment += `### Per-Simulation Results\n\n`;
              comment += `| Simulation | Requests | Success Rate | Mean Time | P95 | P99 |\n`;
              comment += `|------------|----------|--------------|-----------|-----|-----|\n`;
              
              const simLines = simulationStats.split('\n').filter(line => line.trim() !== '');
              for (const line of simLines) {
                const [simName, statsStr] = line.split(':');
                if (statsStr) {
                  const [total, ok, ko, success_rate, mean_time, p95, p99] = statsStr.split('|');
                  
                  // Format simulation name (remove timestamp suffix for readability)
                  let displayName = simName.replace(/simulation-\d+/i, '').replace(/-/g, ' ').trim();
                  if (displayName.toLowerCase().includes('smoketest')) {
                    displayName = 'ðŸ”¥ Smoke Test';
                  } else if (displayName.toLowerCase().includes('comprehensive')) {
                    displayName = 'ðŸ“‹ Comprehensive';
                  } else if (displayName.toLowerCase().includes('userjourney')) {
                    displayName = 'ðŸ‘¤ User Journey';
                  } else if (displayName.toLowerCase().includes('stresstest')) {
                    displayName = 'ðŸ’ª Stress Test';
                  } else if (displayName.toLowerCase().includes('inferno')) {
                    displayName = 'ðŸ”¥ Inferno';
                  }
                  
                  comment += `| ${displayName} | ${total} | ${success_rate}% | ${mean_time}ms | ${p95}ms | ${p99}ms |\n`;
                }
              }
              comment += `\n`;
            }
            
            // Add link to detailed report
            const url = `https://${context.repo.owner}.github.io/${context.repo.repo}/reports/gatling-reports-pr-${context.issue.number}/`;
            comment += `ðŸ“ˆ [View detailed performance reports](${url})\n\n`;
            
            // Add performance criteria info
            comment += `<details>\n<summary>Performance Criteria</summary>\n\n`;
            comment += `- **Smoke Test**: >75% success rate, <5s max response time, <1.5s mean\n`;
            comment += `- **Comprehensive**: >75% success rate, <8s max response time, <2s mean\n`;
            comment += `- **User Journey**: >75% success rate, <10s max response time, <3s mean\n`;
            comment += `- **Stress Test**: >70% success rate, <15s max response time, <5s mean\n`;
            comment += `- **Inferno**: >15% success rate (extreme load scenario)\n`;
            comment += `</details>\n\n`;
            
            // Add marker to identify our comment
            comment += `<!-- gatling-results-comment -->`;
            
            // Find existing comment and update it, or create new one
            const comments = await github.paginate(
              github.rest.issues.listComments,
              {
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
              }
            );
            
            const botComment = comments.find(comment => 
              comment.body.includes('<!-- gatling-results-comment -->')
            );
            
            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            }

  clean:
    needs: gatling
    if: always()
    runs-on: ubuntu-latest
    permissions:
      actions: write
    steps:
      - name: Cleanup old artifacts
        if: github.event_name != 'pull_request'
        uses: actions/github-script@v8
        with:
          # language=JavaScript
          script: |
            const maxVersions = 5;
            const response = await github.rest.actions.listArtifactsForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              per_page: 100
            });
            const artifacts = response.data.artifacts
              .filter(artifact => artifact.name.startsWith('gatling-reports-' + '${{ github.ref_name }}'));
            if (artifacts.length > maxVersions) {
              // Sort by created_at descending
              artifacts.sort((a, b) => new Date(b.created_at) - new Date(a.created_at));
              // Delete all but the latest maxVersions
              for (const artifact of artifacts.slice(maxVersions)) {
                console.log('Deleting artifact:', artifact.name);
                await github.rest.actions.deleteArtifact({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  artifact_id: artifact.id
                });
              }
            }
